{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing the title situation prior to the merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''old_titles_and_variations = {\n",
    "    \"bar_data.csv\": [\n",
    "        \"South Korea leads internet speed, HK and Japan follow\",\n",
    "        \"average internet speeds in asia\"\n",
    "    ],\n",
    "    \"line_data.csv\": [\n",
    "        \"monthly oil price history in 2015\",\n",
    "        \"Oil prices spike between April and June\"\n",
    "    ],\n",
    "    \"pie_data.csv\": [\n",
    "        \"global smartphone market share (%)\",\n",
    "        \"Samsung leads, Apple second in global marketshare of phones\"\n",
    "    ],\n",
    "    \"scatterplot_data.csv\": [\n",
    "        \"height vs. weight of 85 males\",\n",
    "        \"Slightly positive corr between men's height and weight\"\n",
    "    ],\n",
    "    \"stacked_area_data.csv\": [\n",
    "        \"popular girls' names in the uk\",\n",
    "        \"Amelia leads girls names for almost a decade\"\n",
    "    ],\n",
    "    \"stacked_bar_data.csv\": [\n",
    "        \"hotel costs of room service\",\n",
    "        \"NY and Vegas have most expensive room services, Seattle cheapest\"\n",
    "    ],\n",
    "    \"histogram_data.csv\": [\n",
    "        \"taxi passenger ratings\",\n",
    "        \"4.4 to 4.6 Are The Most Common Taxi Passenger Ratings\"\n",
    "    ],\n",
    "    \"area_data.csv\": [\n",
    "        \"average coffee bean price from 2013 to 2014\",\n",
    "        \"Coffe bean price dropped from 2013 high to 2014 low\"\n",
    "        Coffee Bean Price Dropped from 2013 High to 2014 Low\n",
    "    ],\n",
    "    \"100_stacked_bar_data.csv\": [\"election exit poll of california state by education\",\n",
    "                                 \"Democrats in CA shine with no diplomas and postgrads\"]\n",
    "}\n",
    "\n",
    "new_titles_with_variations = {\n",
    "    \"bar_data.csv\": [\n",
    "        \"South Korea Leads Internet Speed, HK and Japan follow\",\n",
    "        \"Average Internet Speeds in Asia\"\n",
    "    ],\n",
    "    \"line_data.csv\": [\n",
    "        \"Monthly Oil Price History in 2015\",\n",
    "        \"Oil Prices Spike Between April and June\"\n",
    "    ],\n",
    "    \"pie_data.csv\": [\n",
    "        \"Global Smartphone Market Share (%)\",\n",
    "        \"Samsung Leads, Apple Second in Global Phone Market Share\"\n",
    "    ],\n",
    "    \"scatterplot_data.csv\": [\n",
    "        \"Height vs. Weight of 85 Males\",\n",
    "        \"Slightly Positive Correlation Between Men's Height and Weight\"\n",
    "    ],\n",
    "    \"stacked_area_data.csv\": [\n",
    "        \"Popular Girls' Names in the UK\",\n",
    "        \"Amelia Leads Girls Names for Almost a Decade\"\n",
    "    ],\n",
    "    \"stacked_bar_data.csv\": [\n",
    "        \"Hotel Costs of Room Service\",\n",
    "        \"NY and Vegas Have Most Expensive Room Services, Seattle Cheapest\"\n",
    "    ],\n",
    "    \"histogram_data.csv\": [\n",
    "        \"Taxi Passenger Ratings\",\n",
    "        \"4.4 to 4.6 Are The Most Common Taxi Passenger Ratings\"\n",
    "    ],\n",
    "    \"area_data.csv\": [\n",
    "        \"Average Coffee Bean Price from 2013 to 2014\",\n",
    "        \"Coffee Bean Price Dropped from 2013 High to 2014 Low\"\n",
    "    ],\n",
    "    \"100_stacked_bar_data.csv\": [\"Election Exit Poll of California State by Education\",\n",
    "                                 \"Democrats in CA Shine With No Diplomas and Postgrads\"]\n",
    "}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_dataframe(data_dict):\n",
    "    \"\"\"\n",
    "    Converts a dictionary with 2-entry lists as values into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data_dict (dict): A dictionary where each value is a 2-entry list.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with columns 'old' and 'new'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = []\n",
    "        for key, values in data_dict.items():\n",
    "            if len(values) != 2:\n",
    "                raise ValueError(f\"Value for key '{key}' is not a 2-entry list.\")\n",
    "            \n",
    "            old_val = \"\"\n",
    "            new_val = \"\"\n",
    "\n",
    "            if isinstance(values[0], str) and values[0].islower():\n",
    "                old_val = values[0]\n",
    "                new_val = values[1]\n",
    "            elif isinstance(values[1], str) and values[1].islower():\n",
    "                old_val = values[1]\n",
    "                new_val = values[0]\n",
    "            else:\n",
    "                raise ValueError(f\"No lowercase string found in value for key '{key}'.\")\n",
    "            \n",
    "            data.append({'old': old_val, 'new': new_val})\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    except (ValueError, TypeError) as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return pd.DataFrame() #Return empty dataframe if error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below is the merging, but above well fix the titles situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_perfect_plots = pd.read_csv(\"finalRESULTSDATASETS/google_correct_plots.csv\")\n",
    "\n",
    "google_perfect_plots_wo_cols = google_perfect_plots.drop(['Unnamed: 0.2', 'Unnamed: 0.1', 'Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so first line of attack, we want to take out response times as well and make a single df for main running results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_perfect_plots_wo_cols_no = google_perfect_plots_wo_cols.drop(['response_time_1',\n",
    "        'response_time_2',  'response_time_3',\n",
    "        'response_time_4',  'response_time_5',\n",
    "        'response_time_6',  'response_time_7',\n",
    "        'response_time_8',  'response_time_9',\n",
    "        'response_time_10', \n",
    "       'response_time_11',  'response_time_12',\n",
    "        'response_time_13', \n",
    "       'response_time_14',  'response_time_15',\n",
    "        'response_time_16', \n",
    "       'response_time_17', 'response_time_18',\n",
    "        'response_time_19',\n",
    "       'response_time_20'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_perfect_plots_wo_cols_no[\"worsened_axes\"] = 0\n",
    "google_perfect_plots_wo_cols_no[\"google_model\"] = 1 #lets make these two ready\n",
    "google_perfect_plots_wo_cols_no[\"openai_model\"] = 0 #lets make these two ready so that i can use the reference datasets for test8 and for perfectaxis separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_images_are_unique(df):\n",
    "    # Group by the two columns and take the first row of each group\n",
    "    unique_combinations_df = df.groupby(['worsened_axes', 'image']).first().reset_index()[['worsened_axes', 'image']]\n",
    "\n",
    "    # Convert to list of tuples (optional)\n",
    "    unique_combinations_list = list(zip(unique_combinations_df['worsened_axes'], unique_combinations_df['image']))\n",
    "\n",
    "    print(unique_combinations_df)\n",
    "    print(unique_combinations_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now for the ones which had the worsened axes, and then we merge them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_worse_axes = pd.read_csv(\"finalRESULTSDATASETS/google_test8.csv\")\n",
    "google_worse_axes_dropped = google_worse_axes.drop(['Unnamed: 0.2', 'Unnamed: 0.1', 'Unnamed: 0','response_time_1',\n",
    "        'response_time_2',  'response_time_3',\n",
    "        'response_time_4',  'response_time_5',\n",
    "        'response_time_6',  'response_time_7',\n",
    "        'response_time_8',  'response_time_9',\n",
    "        'response_time_10', \n",
    "       'response_time_11',  'response_time_12',\n",
    "        'response_time_13', \n",
    "       'response_time_14',  'response_time_15',\n",
    "        'response_time_16', \n",
    "       'response_time_17', 'response_time_18',\n",
    "        'response_time_19',\n",
    "       'response_time_20'],axis=1)\n",
    "google_worse_axes_dropped[\"worsened_axes\"] = 1\n",
    "google_worse_axes_dropped[\"google_model\"] = 1 #lets make these two ready\n",
    "google_worse_axes_dropped[\"openai_model\"] = 0 #lets make these two ready so that i can use the reference datasets for test8 and for perfectaxis separately\n",
    "\n",
    "#checked that both these dfs have 380 combinations of image and worsened axes, and now their merger should also have that actually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge \n",
    "because some of the titles had caps, the images were doubling in both google and openai merges, so before we merge i must just lower caps everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so all we need now is to also change the title of the marketshare, change coffe to coffee and change corr to correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#google_worse_axes_dropped[\"image\"] = google_worse_axes_dropped[\"image\"].str.lower() \n",
    "#google_perfect_plots_wo_cols_no[\"image\"] = google_perfect_plots_wo_cols_no[\"image\"].str.lower()\n",
    "\n",
    "#googledf = pd.concat([google_worse_axes_dropped, google_perfect_plots_wo_cols_no], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#googledf[\"google_model\"] = 1\n",
    "#googledf[\"openai_model\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no need to use googledf - we'll use google_perfect_plots_wo_cols_no for pp and google_worse_axes_dropped for test8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but also do the openai ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_perfect_plots_toomanycols = pd.read_csv(\"finalRESULTSDATASETS/gpt_perfect_plots.csv\")\n",
    "gpt_perfect_plots_wocols = gpt_perfect_plots_toomanycols.drop(['Unnamed: 0','response_time_1',\n",
    "        'response_time_2',  'response_time_3',\n",
    "        'response_time_4',  'response_time_5',\n",
    "        'response_time_6',  'response_time_7',\n",
    "        'response_time_8',  'response_time_9',\n",
    "        'response_time_10', \n",
    "       'response_time_11',  'response_time_12',\n",
    "        'response_time_13', \n",
    "       'response_time_14',  'response_time_15',\n",
    "        'response_time_16', \n",
    "       'response_time_17', 'response_time_18',\n",
    "        'response_time_19',\n",
    "       'response_time_20'],axis=1)\n",
    "\n",
    "gpt_perfect_plots_wocols[\"worsened_axes\"] = 0\n",
    "gpt_perfect_plots_wocols[\"google_model\"] = 0\n",
    "gpt_perfect_plots_wocols[\"openai_model\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_Test8 = pd.read_csv(\"finalRESULTSDATASETS/gpt_test8.csv\")\n",
    "gpt_test8_Ready = gpt_Test8.drop('Unnamed: 0', axis = 1)\n",
    "gpt_test8_Ready[\"google_model\"] = 0\n",
    "gpt_test8_Ready[\"openai_model\"] = 1\n",
    "gpt_test8_Ready[\"worsened_axes\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "theres need to change the \"cannot infer\" in some histogram files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now we have gpt_test8_Ready and gpt_perfect_plots_wocols for the test8 and pp from gpt - this way id have a ready dataset with the worsened axes by merging gpt_test8_Ready and google_worse_axes_dropped and a perfect dataset by merging google_perfect_plots_wo_cols and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#openaidf = pd.concat([gpt_perfect_plots_wocols, gpt_test8_Ready], ignore_index=True)\n",
    "#openaidf[\"google_model\"] = 0\n",
    "#openaidf[\"openai_model\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#openai_google_combined = pd.concat([openaidf, googledf], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#openai_google_combined_valuesreplaced2.to_csv(\"testingTHEFINAL_correctsize_rightvars.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we double checked the size, each formative one has 1820 rows! amazing, lets create a function to craft all the other vars!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i need to use two different datasets, one for the old titles of test8 and one for the new titles of the perfect plots to match with all the characteristics of these plots, and then i can just create the dummies once we have the test8 and the perfectplots' reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)creating the perfect plots reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#this is the generator of a dataset based on a config, but i need the new configs\n",
    "def generate_dataset(config_list):\n",
    "    records = []\n",
    "    \n",
    "    with open(config_list, \"r\") as file:\n",
    "        ready_config_list = json.load(file)\n",
    "    for config in ready_config_list:\n",
    "        \n",
    "        filename = f\"{config['dataset']}_{config['plot_type']}_{config['title'].replace(' ', '_')}_{config['color'][0]}.png\"\n",
    "        \n",
    "        record = {\n",
    "            \"filename\": filename,\n",
    "            \"dataset\": config[\"dataset\"],\n",
    "            \"plot_type\": config[\"plot_type\"],\n",
    "            \"title\": config[\"title\"],\n",
    "            \"color\": config[\"color\"][0]\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now i need the specific config.json for the new plots -NOT THE OLD ONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GETTING THE NEW PLOTS CONFIG.JSON\n",
    "#i used that old function to generate something called newConfigs.json, that we will now feed to the generate dataset\n",
    "\n",
    "# mkdir cell that uses a specific generate_config and save_plot functions (depending on the save_plot and the titles different configs will be saved) for newConfigs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now with the newConfigs, ill generate the dataset that would allow me to fetch out specific dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dataset_for_dummy_generation = generate_dataset(\"newConfigs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>dataset</th>\n",
       "      <th>plot_type</th>\n",
       "      <th>title</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>line_data.csv_line_Monthly_Oil_Price_History_i...</td>\n",
       "      <td>line_data.csv</td>\n",
       "      <td>line</td>\n",
       "      <td>Monthly Oil Price History in 2015</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>line_data.csv_line_Oil_Prices_Spike_Between_Ap...</td>\n",
       "      <td>line_data.csv</td>\n",
       "      <td>line</td>\n",
       "      <td>Oil Prices Spike Between April and June</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>line_data.csv_line_Monthly_Oil_Price_History_i...</td>\n",
       "      <td>line_data.csv</td>\n",
       "      <td>line</td>\n",
       "      <td>Monthly Oil Price History in 2015</td>\n",
       "      <td>saddlebrown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>line_data.csv_line_Oil_Prices_Spike_Between_Ap...</td>\n",
       "      <td>line_data.csv</td>\n",
       "      <td>line</td>\n",
       "      <td>Oil Prices Spike Between April and June</td>\n",
       "      <td>saddlebrown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>line_data.csv_line_Monthly_Oil_Price_History_i...</td>\n",
       "      <td>line_data.csv</td>\n",
       "      <td>line</td>\n",
       "      <td>Monthly Oil Price History in 2015</td>\n",
       "      <td>lightgray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>stacked_area_data.csv_stacked_bar_100_Amelia_L...</td>\n",
       "      <td>stacked_area_data.csv</td>\n",
       "      <td>stacked_bar_100</td>\n",
       "      <td>Amelia Leads Girls Names for Almost a Decade</td>\n",
       "      <td>mediumvioletred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>stacked_area_data.csv_stacked_bar_100_Popular_...</td>\n",
       "      <td>stacked_area_data.csv</td>\n",
       "      <td>stacked_bar_100</td>\n",
       "      <td>Popular Girls' Names in the UK</td>\n",
       "      <td>chartreuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>stacked_area_data.csv_stacked_bar_100_Amelia_L...</td>\n",
       "      <td>stacked_area_data.csv</td>\n",
       "      <td>stacked_bar_100</td>\n",
       "      <td>Amelia Leads Girls Names for Almost a Decade</td>\n",
       "      <td>chartreuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>stacked_area_data.csv_stacked_bar_100_Popular_...</td>\n",
       "      <td>stacked_area_data.csv</td>\n",
       "      <td>stacked_bar_100</td>\n",
       "      <td>Popular Girls' Names in the UK</td>\n",
       "      <td>burlywood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>stacked_area_data.csv_stacked_bar_100_Amelia_L...</td>\n",
       "      <td>stacked_area_data.csv</td>\n",
       "      <td>stacked_bar_100</td>\n",
       "      <td>Amelia Leads Girls Names for Almost a Decade</td>\n",
       "      <td>burlywood</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>380 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filename                dataset  \\\n",
       "0    line_data.csv_line_Monthly_Oil_Price_History_i...          line_data.csv   \n",
       "1    line_data.csv_line_Oil_Prices_Spike_Between_Ap...          line_data.csv   \n",
       "2    line_data.csv_line_Monthly_Oil_Price_History_i...          line_data.csv   \n",
       "3    line_data.csv_line_Oil_Prices_Spike_Between_Ap...          line_data.csv   \n",
       "4    line_data.csv_line_Monthly_Oil_Price_History_i...          line_data.csv   \n",
       "..                                                 ...                    ...   \n",
       "375  stacked_area_data.csv_stacked_bar_100_Amelia_L...  stacked_area_data.csv   \n",
       "376  stacked_area_data.csv_stacked_bar_100_Popular_...  stacked_area_data.csv   \n",
       "377  stacked_area_data.csv_stacked_bar_100_Amelia_L...  stacked_area_data.csv   \n",
       "378  stacked_area_data.csv_stacked_bar_100_Popular_...  stacked_area_data.csv   \n",
       "379  stacked_area_data.csv_stacked_bar_100_Amelia_L...  stacked_area_data.csv   \n",
       "\n",
       "           plot_type                                         title  \\\n",
       "0               line             Monthly Oil Price History in 2015   \n",
       "1               line       Oil Prices Spike Between April and June   \n",
       "2               line             Monthly Oil Price History in 2015   \n",
       "3               line       Oil Prices Spike Between April and June   \n",
       "4               line             Monthly Oil Price History in 2015   \n",
       "..               ...                                           ...   \n",
       "375  stacked_bar_100  Amelia Leads Girls Names for Almost a Decade   \n",
       "376  stacked_bar_100                Popular Girls' Names in the UK   \n",
       "377  stacked_bar_100  Amelia Leads Girls Names for Almost a Decade   \n",
       "378  stacked_bar_100                Popular Girls' Names in the UK   \n",
       "379  stacked_bar_100  Amelia Leads Girls Names for Almost a Decade   \n",
       "\n",
       "               color  \n",
       "0                red  \n",
       "1                red  \n",
       "2        saddlebrown  \n",
       "3        saddlebrown  \n",
       "4          lightgray  \n",
       "..               ...  \n",
       "375  mediumvioletred  \n",
       "376       chartreuse  \n",
       "377       chartreuse  \n",
       "378        burlywood  \n",
       "379        burlywood  \n",
       "\n",
       "[380 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dataset_for_dummy_generation.to_csv(\"newConfigsReferenceDataset.csv\")\n",
    "config_dataset_for_dummy_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll go on to make every column a fucking dummy: the title is no longer divided into caps and no caps, so ill need to use a dictionary but the rest i can use the pd.dummy function! The pipeline for the other referenceDatasetForGeneratedPlots.csv is the pipeline for the test8 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ill just create a dict with what is a referential title and what is a normal one\n",
    "dataset_plot_title_with_variations2 = {\n",
    "    \"bar_data.csv\": [\n",
    "        \"Average Internet Speeds in Asia\",\n",
    "        \"South Korea Leads Internet Speed, HK and Japan follow\"\n",
    "    ],\n",
    "    \"line_data.csv\": [\n",
    "        \"Monthly Oil Price History in 2015\",\n",
    "        \"Oil Prices Spike Between April and June\"\n",
    "    ],\n",
    "    \"pie_data.csv\": [\n",
    "        \"Global Smartphone Market Share (%)\",\n",
    "        \"Samsung Leads, Apple Second in Global Phone Market Share\"\n",
    "    ],\n",
    "    \"scatterplot_data.csv\": [\n",
    "        \"Height vs. Weight of 85 Males\",\n",
    "        \"Slightly Positive Correlation Between Men's Height and Weight\"\n",
    "    ],\n",
    "    \"stacked_area_data.csv\": [\n",
    "        \"Popular Girls' Names in the UK\",\n",
    "        \"Amelia Leads Girls Names for Almost a Decade\"\n",
    "    ],\n",
    "    \"stacked_bar_data.csv\": [\n",
    "        \"Hotel Costs of Room Service\",\n",
    "        \"NY and Vegas Have Most Expensive Room Services, Seattle Cheapest\"\n",
    "    ],\n",
    "    \"histogram_data.csv\": [\n",
    "        \"Taxi Passenger Ratings\",\n",
    "        \"4.4 to 4.6 Are The Most Common Taxi Passenger Ratings\"\n",
    "    ],\n",
    "    \"area_data.csv\": [\n",
    "        \"Average Coffee Bean Price from 2013 to 2014\",\n",
    "        \"Coffee Bean Price Dropped from 2013 High to 2014 Low\"\n",
    "    ],\n",
    "    \"100_stacked_bar_data.csv\": [\"Election Exit Poll of California State by Education\",\n",
    "                                 \"Democrats in CA Shine With No Diplomas and Postgrads\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_titles = []\n",
    "referential_titles = []\n",
    "\n",
    "for i in dataset_plot_title_with_variations2.values():\n",
    "    normal_titles.append(i[0])\n",
    "    referential_titles.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now i just make that a dataframe\n",
    "mid_data1 = {\"referential_title\": 1, \"title\": referential_titles}\n",
    "mid_data2 = {\"referential_title\": 0, \"title\": normal_titles}\n",
    "\n",
    "referential_titles_df1 = pd.DataFrame(mid_data1)\n",
    "referential_titles_df2 = pd.DataFrame(mid_data2)\n",
    "\n",
    "referential_titles_df = pd.concat([referential_titles_df1, referential_titles_df2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now ill figure out the title for the configs, and then extend the other dummies, then ill merge it with the perfect axes df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dataset_for_dummy_generation_with_titles = pd.merge(referential_titles_df, config_dataset_for_dummy_generation, \"left\")\n",
    "#config_dataset_for_dummy_generation_with_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now we generate dummies for the rest before merging with perfect axes df (google perfect axes+openai perfect axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummies_for_columns(df, columns):\n",
    "    \"\"\"\n",
    "    Creates dummy variables for specified columns in a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "      df: The Pandas DataFrame.\n",
    "      columns: A list of column names to create dummies for.\n",
    "\n",
    "    Returns:\n",
    "      The DataFrame with dummy variables added, and original columns removed.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        dummies = pd.get_dummies(df[col], prefix=col)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        df = df.drop(col, axis=1)  # Remove the original column\n",
    "    return df\n",
    "\n",
    "\n",
    "predf_to_be_merged_with_perfect_axes = create_dummies_for_columns(config_dataset_for_dummy_generation_with_titles, ['dataset', 'plot_type', 'color'])\n",
    "df_to_be_merged_with_perfect_axes = predf_to_be_merged_with_perfect_axes.drop(\"title\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      bar_data.csv_bar_South_Korea_Leads_Internet_Sp...\n",
       "1      bar_data.csv_bar_South_Korea_Leads_Internet_Sp...\n",
       "2      bar_data.csv_bar_South_Korea_Leads_Internet_Sp...\n",
       "3      bar_data.csv_bar_South_Korea_Leads_Internet_Sp...\n",
       "4      bar_data.csv_bar_South_Korea_Leads_Internet_Sp...\n",
       "                             ...                        \n",
       "375    100_stacked_bar_data.csv_stacked_bar_100_Elect...\n",
       "376    100_stacked_bar_data.csv_stacked_bar_100_Elect...\n",
       "377    100_stacked_bar_data.csv_stacked_bar_100_Elect...\n",
       "378    100_stacked_bar_data.csv_stacked_bar_100_Elect...\n",
       "379    100_stacked_bar_data.csv_stacked_bar_100_Elect...\n",
       "Name: filename, Length: 380, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_be_merged_with_perfect_axes[\"filename\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we actually merge with the stacking of the two datasets from the perfect axes\n",
    "perfect_axes = pd.concat([gpt_perfect_plots_wocols, google_perfect_plots_wo_cols_no])\n",
    "perfect_axes[\"filename\"] = perfect_axes[\"image\"] \n",
    "perfect_axes_premerger = perfect_axes.drop([\"dataset\", \"image\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_axes_wdummies = pd.merge(df_to_be_merged_with_perfect_axes, perfect_axes_premerger, \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_axes_wdummies.to_csv(\"perfect_axes_wdummies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using this as a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "perfect_axes_wdummies = pd.read_csv(\"perfect_axes_wdummies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now lets do the process of cleaning with the answers to create our design guidelines dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#lets grab the answers csv\n",
    "ans = pd.read_csv(\"ans.csv\")\n",
    "trace2 = ans.iloc[12][\"question\"] #problematic question\n",
    "trace = ans.iloc[40][\"question\"] #problematic question2\n",
    "\n",
    "#define a function to remove these questions from dataframes\n",
    "def filter_dataframe_without_traces_normalized(df, trace1, trace2):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame to exclude rows where 'question' matches trace1 or trace2 \n",
    "    (case-insensitive, no spaces) after normalizing the 'question' column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Compile regular expression for removing spaces\n",
    "        space_re = re.compile(r'\\s+')\n",
    "\n",
    "        # Normalize trace strings\n",
    "        trace1_normalized = space_re.sub('', str(trace1).lower())\n",
    "        trace2_normalized = space_re.sub('', str(trace2).lower())\n",
    "\n",
    "        # Normalize the 'question' column for comparison\n",
    "        normalized_question = df['question'].astype(str).str.lower().apply(lambda x: space_re.sub('', x))\n",
    "\n",
    "        # Apply filtering based on the normalized 'question' column\n",
    "        filtered_df = df[\n",
    "            (normalized_question != trace1_normalized) &\n",
    "            (normalized_question != trace2_normalized)\n",
    "        ]\n",
    "        return filtered_df\n",
    "    except KeyError:\n",
    "        print(\"Error: 'question' column not found in DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "    except AttributeError:\n",
    "        print(\"Error: one of the traces is not a string or cannot be converted to a string\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#droppping the problematic questions from perfect_axes_wdummies\n",
    "perfect_axes_wdummies_nobadq = filter_dataframe_without_traces_normalized(perfect_axes_wdummies, trace, trace2)\n",
    "#perfect_axes_wdummies_nobadq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the problematic and the ones we didnt run from ans as well so we could merge these two\n",
    "ans_wo_problematic = ans.drop([12,40])\n",
    "new_ans_for_perfect = ans_wo_problematic.drop(range(43,54))\n",
    "ready_ans = ans.drop(range(43,54))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "leave here the full questions without taking out the problematic questions for general assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perfect_axes_wdummies and ans\n",
    "#now we merge them by question, but we should probably first take out all spaces and highletters from the questions\n",
    "perfect_axes_wdummies[\"question_lowered_nospace\"] = perfect_axes_wdummies[\"question\"].astype(str).str.lower().str.replace(r'\\s+', '', regex=True)\n",
    "ready_ans[\"question_lowered_nospace\"] = ready_ans[\"question\"].astype(str).str.lower().str.replace(r'\\s+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlat_ex = pd.merge(ready_ans, perfect_axes_wdummies, \"left\", \"question_lowered_nospace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0_x', 'dataset', 'question_x', 'answer', 'question_type',\n",
       "       'question_lowered_nospace', 'Unnamed: 0_y', 'referential_title',\n",
       "       'filename', 'dataset_100_stacked_bar_data.csv', 'dataset_area_data.csv',\n",
       "       'dataset_bar_data.csv', 'dataset_histogram_data.csv',\n",
       "       'dataset_line_data.csv', 'dataset_pie_data.csv',\n",
       "       'dataset_scatterplot_data.csv', 'dataset_stacked_area_data.csv',\n",
       "       'dataset_stacked_bar_data.csv', 'plot_type_bar', 'plot_type_hist',\n",
       "       'plot_type_line', 'plot_type_pie', 'plot_type_scatter',\n",
       "       'plot_type_stacked_bar', 'plot_type_stacked_bar_100', 'color_black',\n",
       "       'color_burlywood', 'color_chartreuse', 'color_lightgray',\n",
       "       'color_lightpink', 'color_mediumvioletred', 'color_navy', 'color_red',\n",
       "       'color_saddlebrown', 'color_tomato', 'question_y', 'gpt_answer_1',\n",
       "       'gpt_answer_2', 'gpt_answer_3', 'gpt_answer_4', 'gpt_answer_5',\n",
       "       'gpt_answer_6', 'gpt_answer_7', 'gpt_answer_8', 'gpt_answer_9',\n",
       "       'gpt_answer_10', 'gpt_answer_11', 'gpt_answer_12', 'gpt_answer_13',\n",
       "       'gpt_answer_14', 'gpt_answer_15', 'gpt_answer_16', 'gpt_answer_17',\n",
       "       'gpt_answer_18', 'gpt_answer_19', 'gpt_answer_20', 'worsened_axes',\n",
       "       'google_model', 'openai_model'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vlat_ex.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlat_ex2 = vlat_ex.drop(['Unnamed: 0_x', 'Unnamed: 0_y','question_y', 'gpt_answer_1',\n",
    "       'gpt_answer_2', 'gpt_answer_3', 'gpt_answer_4', 'gpt_answer_5',\n",
    "       'gpt_answer_6', 'gpt_answer_7', 'gpt_answer_8', 'gpt_answer_9',\n",
    "       'gpt_answer_10', 'gpt_answer_11', 'gpt_answer_12', 'gpt_answer_13',\n",
    "       'gpt_answer_14', 'gpt_answer_15', 'gpt_answer_16', 'gpt_answer_17',\n",
    "       'gpt_answer_18', 'gpt_answer_19', 'gpt_answer_20', 'worsened_axes',\n",
    "       'google_model', 'openai_model', 'question_lowered_nospace'], axis=1)\n",
    "vlat_ex3 = vlat_ex2.rename(columns={\"question_x\":\"question\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlat_ex3.to_csv(\"additional_materials/vlatex.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECK: NEW_ANS AND perfect_axes_wdummies_nobadq SHOULD HAVE SAME AMOUNT OF DIFFERENT QUESTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(perfect_axes_wdummies_nobadq[\"question\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_ans_for_perfect[\"question\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matheus\\AppData\\Local\\Temp\\ipykernel_77864\\1863128538.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  perfect_axes_wdummies_nobadq[\"question_lowered_nospace\"] = perfect_axes_wdummies_nobadq[\"question\"].astype(str).str.lower().str.replace(r'\\s+', '', regex=True)\n"
     ]
    }
   ],
   "source": [
    "#now we merge them by question, but we should probably first take out all spaces and highletters from the questions\n",
    "perfect_axes_wdummies_nobadq[\"question_lowered_nospace\"] = perfect_axes_wdummies_nobadq[\"question\"].astype(str).str.lower().str.replace(r'\\s+', '', regex=True)\n",
    "new_ans_for_perfect[\"question_lowered_nospace\"] = new_ans_for_perfect[\"question\"].astype(str).str.lower().str.replace(r'\\s+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_axes_wdummies_nobadq_wans = pd.merge(new_ans_for_perfect, perfect_axes_wdummies_nobadq, \"left\", \"question_lowered_nospace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we just trim down the columns and the responses, so we can count the rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(perfect_axes_wdummies_nobadq_wans[\"filename\"].unique())\n",
    "perfect_axes_wdummies_nobadq_wans[\"question\"] = perfect_axes_wdummies_nobadq_wans[\"question_x\"]\n",
    "perfect_axes_wdummies_nobadq_wans.drop([\"Unnamed: 0_x\", \"Unnamed: 0_y\", \"question_x\",\"question_y\", \"filename\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dataset', 'answer', 'question_type', 'question_lowered_nospace',\n",
       "       'referential_title', 'dataset_100_stacked_bar_data.csv',\n",
       "       'dataset_area_data.csv', 'dataset_bar_data.csv',\n",
       "       'dataset_histogram_data.csv', 'dataset_line_data.csv',\n",
       "       'dataset_pie_data.csv', 'dataset_scatterplot_data.csv',\n",
       "       'dataset_stacked_area_data.csv', 'dataset_stacked_bar_data.csv',\n",
       "       'plot_type_bar', 'plot_type_hist', 'plot_type_line', 'plot_type_pie',\n",
       "       'plot_type_scatter', 'plot_type_stacked_bar',\n",
       "       'plot_type_stacked_bar_100', 'color_black', 'color_burlywood',\n",
       "       'color_chartreuse', 'color_lightgray', 'color_lightpink',\n",
       "       'color_mediumvioletred', 'color_navy', 'color_red', 'color_saddlebrown',\n",
       "       'color_tomato', 'gpt_answer_1', 'gpt_answer_2', 'gpt_answer_3',\n",
       "       'gpt_answer_4', 'gpt_answer_5', 'gpt_answer_6', 'gpt_answer_7',\n",
       "       'gpt_answer_8', 'gpt_answer_9', 'gpt_answer_10', 'gpt_answer_11',\n",
       "       'gpt_answer_12', 'gpt_answer_13', 'gpt_answer_14', 'gpt_answer_15',\n",
       "       'gpt_answer_16', 'gpt_answer_17', 'gpt_answer_18', 'gpt_answer_19',\n",
       "       'gpt_answer_20', 'worsened_axes', 'google_model', 'openai_model',\n",
       "       'question', 'correct_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perfect_axes_wdummies_nobadq_wans.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we must change the correct answers and the given answers so we can count stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop for trimming down all the answers\n",
    "for col in ['gpt_answer_1', 'gpt_answer_2', 'gpt_answer_3',\n",
    "       'gpt_answer_4', 'gpt_answer_5', 'gpt_answer_6', 'gpt_answer_7',\n",
    "       'gpt_answer_8', 'gpt_answer_9', 'gpt_answer_10', 'gpt_answer_11',\n",
    "       'gpt_answer_12', 'gpt_answer_13', 'gpt_answer_14', 'gpt_answer_15',\n",
    "       'gpt_answer_16', 'gpt_answer_17', 'gpt_answer_18', 'gpt_answer_19',\n",
    "       'gpt_answer_20', 'answer']:\n",
    "      if col in perfect_axes_wdummies_nobadq_wans.columns: #Check that the column exists.\n",
    "        perfect_axes_wdummies_nobadq_wans[col] = perfect_axes_wdummies_nobadq_wans[col].astype(str).str.lower().str.replace(r'\\s+', '', regex=True)\n",
    "\n",
    "\n",
    "#function for getting the count of columns that match the correct answer\n",
    "def add_correct_count_column(df, answer_columns, correct_answer_column, new_column_name='correct_count'):\n",
    "    \"\"\"\n",
    "    Adds a new column to the DataFrame containing the number of answer columns that match the correct answer.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to modify.\n",
    "        answer_columns (list): A list of column names representing answer choices.\n",
    "        correct_answer_column (str): The name of the column containing the correct answer.\n",
    "        new_column_name (str): The name of the new column to create. Defaults to 'correct_count'.\n",
    "    \"\"\"\n",
    "\n",
    "    def count_correct(row):\n",
    "        correct_answer = row[correct_answer_column]\n",
    "        correct_count = 0\n",
    "        for col in answer_columns:\n",
    "            if row[col] == correct_answer:\n",
    "                correct_count += 1\n",
    "        return correct_count\n",
    "\n",
    "    df[new_column_name] = df.apply(count_correct, axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_axes_wdummies_nobadq_wans_wres = add_correct_count_column(perfect_axes_wdummies_nobadq_wans, \n",
    "                         ['gpt_answer_1', 'gpt_answer_2', 'gpt_answer_3',\n",
    "       'gpt_answer_4', 'gpt_answer_5', 'gpt_answer_6', 'gpt_answer_7',\n",
    "       'gpt_answer_8', 'gpt_answer_9', 'gpt_answer_10', 'gpt_answer_11',\n",
    "       'gpt_answer_12', 'gpt_answer_13', 'gpt_answer_14', 'gpt_answer_15',\n",
    "       'gpt_answer_16', 'gpt_answer_17', 'gpt_answer_18', 'gpt_answer_19',\n",
    "       'gpt_answer_20'],\"answer\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_axes_wdummies_nobadq_wans_wres.to_csv(\"perfect_axes_results_AM.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this, is a ready dataset for GENERATING RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this below is the test8 pipeline - wed do the same thing just using the other config, so that the titles match when using the referential dataset, i think the other referential dataset would suffice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this should have - and does have 380 images\n",
    "referencepredummy = pd.read_csv(\"referenceDatasetForGeneratedPlots.csv\")\n",
    "len(referencepredummy[\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quick function to categorize the title as either normal or referential (the economist like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def categorize_text(df, text_column):\n",
    "    \"\"\"\n",
    "    Categorizes text entries based on the presence of uppercase letters.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the text column.\n",
    "        text_column (str): The name of the column containing the text.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with 'referential' and 'normal' columns added.\n",
    "    \"\"\"\n",
    "\n",
    "    def check_caps(text):\n",
    "        if isinstance(text, str): #check if entry is a string\n",
    "            if re.search(r'[A-Z]', text):\n",
    "                return 'referential'\n",
    "            else:\n",
    "                return 'normal'\n",
    "        else: #if the entry is not a string, put in normal.\n",
    "            return 'normal'\n",
    "\n",
    "    categories = df[text_column].apply(check_caps)\n",
    "\n",
    "    df['referential_title'] = (categories == 'referential').astype(int)\n",
    "    df['normal_title'] = (categories == 'normal').astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>dataset</th>\n",
       "      <th>plot_type</th>\n",
       "      <th>title</th>\n",
       "      <th>color</th>\n",
       "      <th>referential_title</th>\n",
       "      <th>normal_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>line_data.csv_line_monthly_oil_price_history_i...</td>\n",
       "      <td>line_data.csv</td>\n",
       "      <td>line</td>\n",
       "      <td>monthly oil price history in 2015</td>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>line_data.csv_line_Oil_prices_spike_between_Ap...</td>\n",
       "      <td>line_data.csv</td>\n",
       "      <td>line</td>\n",
       "      <td>Oil prices spike between April and June</td>\n",
       "      <td>red</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>line_data.csv_line_monthly_oil_price_history_i...</td>\n",
       "      <td>line_data.csv</td>\n",
       "      <td>line</td>\n",
       "      <td>monthly oil price history in 2015</td>\n",
       "      <td>saddlebrown</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>line_data.csv_line_Oil_prices_spike_between_Ap...</td>\n",
       "      <td>line_data.csv</td>\n",
       "      <td>line</td>\n",
       "      <td>Oil prices spike between April and June</td>\n",
       "      <td>saddlebrown</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>line_data.csv_line_monthly_oil_price_history_i...</td>\n",
       "      <td>line_data.csv</td>\n",
       "      <td>line</td>\n",
       "      <td>monthly oil price history in 2015</td>\n",
       "      <td>lightgray</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>375</td>\n",
       "      <td>stacked_area_data.csv_stacked_bar_100_Amelia_l...</td>\n",
       "      <td>stacked_area_data.csv</td>\n",
       "      <td>stacked_bar_100</td>\n",
       "      <td>Amelia leads girls names for almost a decade</td>\n",
       "      <td>mediumvioletred</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>376</td>\n",
       "      <td>stacked_area_data.csv_stacked_bar_100_popular_...</td>\n",
       "      <td>stacked_area_data.csv</td>\n",
       "      <td>stacked_bar_100</td>\n",
       "      <td>popular girls' names in the uk</td>\n",
       "      <td>chartreuse</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>377</td>\n",
       "      <td>stacked_area_data.csv_stacked_bar_100_Amelia_l...</td>\n",
       "      <td>stacked_area_data.csv</td>\n",
       "      <td>stacked_bar_100</td>\n",
       "      <td>Amelia leads girls names for almost a decade</td>\n",
       "      <td>chartreuse</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>378</td>\n",
       "      <td>stacked_area_data.csv_stacked_bar_100_popular_...</td>\n",
       "      <td>stacked_area_data.csv</td>\n",
       "      <td>stacked_bar_100</td>\n",
       "      <td>popular girls' names in the uk</td>\n",
       "      <td>burlywood</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>379</td>\n",
       "      <td>stacked_area_data.csv_stacked_bar_100_Amelia_l...</td>\n",
       "      <td>stacked_area_data.csv</td>\n",
       "      <td>stacked_bar_100</td>\n",
       "      <td>Amelia leads girls names for almost a decade</td>\n",
       "      <td>burlywood</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>380 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                           filename  \\\n",
       "0             0  line_data.csv_line_monthly_oil_price_history_i...   \n",
       "1             1  line_data.csv_line_Oil_prices_spike_between_Ap...   \n",
       "2             2  line_data.csv_line_monthly_oil_price_history_i...   \n",
       "3             3  line_data.csv_line_Oil_prices_spike_between_Ap...   \n",
       "4             4  line_data.csv_line_monthly_oil_price_history_i...   \n",
       "..          ...                                                ...   \n",
       "375         375  stacked_area_data.csv_stacked_bar_100_Amelia_l...   \n",
       "376         376  stacked_area_data.csv_stacked_bar_100_popular_...   \n",
       "377         377  stacked_area_data.csv_stacked_bar_100_Amelia_l...   \n",
       "378         378  stacked_area_data.csv_stacked_bar_100_popular_...   \n",
       "379         379  stacked_area_data.csv_stacked_bar_100_Amelia_l...   \n",
       "\n",
       "                   dataset        plot_type  \\\n",
       "0            line_data.csv             line   \n",
       "1            line_data.csv             line   \n",
       "2            line_data.csv             line   \n",
       "3            line_data.csv             line   \n",
       "4            line_data.csv             line   \n",
       "..                     ...              ...   \n",
       "375  stacked_area_data.csv  stacked_bar_100   \n",
       "376  stacked_area_data.csv  stacked_bar_100   \n",
       "377  stacked_area_data.csv  stacked_bar_100   \n",
       "378  stacked_area_data.csv  stacked_bar_100   \n",
       "379  stacked_area_data.csv  stacked_bar_100   \n",
       "\n",
       "                                            title            color  \\\n",
       "0               monthly oil price history in 2015              red   \n",
       "1         Oil prices spike between April and June              red   \n",
       "2               monthly oil price history in 2015      saddlebrown   \n",
       "3         Oil prices spike between April and June      saddlebrown   \n",
       "4               monthly oil price history in 2015        lightgray   \n",
       "..                                            ...              ...   \n",
       "375  Amelia leads girls names for almost a decade  mediumvioletred   \n",
       "376                popular girls' names in the uk       chartreuse   \n",
       "377  Amelia leads girls names for almost a decade       chartreuse   \n",
       "378                popular girls' names in the uk        burlywood   \n",
       "379  Amelia leads girls names for almost a decade        burlywood   \n",
       "\n",
       "     referential_title  normal_title  \n",
       "0                    0             1  \n",
       "1                    1             0  \n",
       "2                    0             1  \n",
       "3                    1             0  \n",
       "4                    0             1  \n",
       "..                 ...           ...  \n",
       "375                  1             0  \n",
       "376                  0             1  \n",
       "377                  1             0  \n",
       "378                  0             1  \n",
       "379                  1             0  \n",
       "\n",
       "[380 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorize_text(referencepredummy,\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummies_for_columns(df, columns):\n",
    "    \"\"\"\n",
    "    Creates dummy variables for specified columns in a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "      df: The Pandas DataFrame.\n",
    "      columns: A list of column names to create dummies for.\n",
    "\n",
    "    Returns:\n",
    "      The DataFrame with dummy variables added, and original columns removed.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        dummies = pd.get_dummies(df[col], prefix=col)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        df = df.drop(col, axis=1)  # Remove the original column\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "referencewDUMMIES = create_dummies_for_columns(referencepredummy, ['dataset', 'plot_type', 'color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "referencewDUMMIES2 = referencewDUMMIES.rename(columns={'filename': 'image'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "referencewDUMMIES2.to_csv(\"referenceDummies_for_test8help.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'referencewDUMMIES2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(referencewDUMMIES2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'referencewDUMMIES2' is not defined"
     ]
    }
   ],
   "source": [
    "len(referencewDUMMIES2[\"image\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before we proceed, its important to adress why theres 7k rows before the merge and only half after - it turns out it was the way we were doing the merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now i need to merge with the answers df that we got with each answer for the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets grab the answers csv\n",
    "ans = pd.read_csv(\"ans.csv\")\n",
    "trace2 = ans.iloc[12][\"question\"]\n",
    "trace = ans.iloc[40][\"question\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i marked these as traces and ill have them removed for now - ill try to run these 4 againand worst case ill use them from what we already have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the cost range of a sandwich in the cities? Options: $0 - $24.2, $0 - $55.9, $13 - $24.2, $17 - $35.2 The cost of vodka in Atlanta is higher than that of Honolulu. Options: True, False, Omit'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace2 = ans.iloc[12][\"question\"]\n",
    "ans.iloc[12][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Over the course of years between 2009 and 2014, what was the range of the number of girls named â€˜Oliviaâ€™? Options: 1200-4700, 1200-8700, 1800-4000, 3000-8700The number of girls named â€˜Islaâ€™ was __________ from 2009 to 2012. Options: rising, falling, staying, Omit'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace = ans.iloc[40][\"question\"]\n",
    "ans.iloc[40][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filled_merged_df['question'].astype(str).str.lower().str.replace(r'\\s+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe_without_traces_normalized(df, trace1, trace2):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame to exclude rows where 'question' matches trace1 or trace2 \n",
    "    (case-insensitive, no spaces) after normalizing the 'question' column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to filter.\n",
    "        trace1 (str): The first trace to exclude.\n",
    "        trace2 (str): The second trace to exclude.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Compile regular expression for removing spaces\n",
    "        space_re = re.compile(r'\\s+')\n",
    "\n",
    "        # Normalize trace strings\n",
    "        trace1_normalized = space_re.sub('', str(trace1).lower())\n",
    "        trace2_normalized = space_re.sub('', str(trace2).lower())\n",
    "\n",
    "        # Normalize the 'question' column for comparison\n",
    "        normalized_question = df['question'].astype(str).str.lower().apply(lambda x: space_re.sub('', x))\n",
    "\n",
    "        # Apply filtering based on the normalized 'question' column\n",
    "        filtered_df = df[\n",
    "            (normalized_question != trace1_normalized) &\n",
    "            (normalized_question != trace2_normalized)\n",
    "        ]\n",
    "        return filtered_df\n",
    "    except KeyError:\n",
    "        print(\"Error: 'question' column not found in DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "    except AttributeError:\n",
    "        print(\"Error: one of the traces is not a string or cannot be converted to a string\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_filled_merged_df = filter_dataframe_without_traces_normalized(filled_merged_df, trace, trace2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_filled_merged_df.to_csv(\"needtocheck.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this ready_filled_merged_df really shouldnt have the problematic question, yet it seems to have! data amirite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this cell below is malfunctioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is not working for some reason so i'll manually drop the questions that are problematic rn and generate results without them\n",
    "#ans_wo_problem_questions = ans[(ans[\"question\"]!= trace) & (ans[\"question\"]!= trace2)]\n",
    "new_ans = ans.drop([12,40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i'll also go ahead and drop all the ones that we didnt use cause i just wanna have a perfectly checked sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ans = new_ans.drop(range(43,54))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now i just merge this with the filled_merged_df but i just want to make sure of the hits, so i'll try to go through the questions here and see how many i find in the filled_merged, cause i should always find a same amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the length of any of them should be non zero, indicating we have that question somewhere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_that_are_not_supposed_to_be_zero = []\n",
    "for i in range(0,len(new_ans)):\n",
    "    #print(new_ans.iloc[i][\"question\"])\n",
    "    n_of_times_question_appears = len(ready_filled_merged_df[ready_filled_merged_df[\"question\"]==new_ans.iloc[i][\"question\"]])\n",
    "    values_that_are_not_supposed_to_be_zero.append(n_of_times_question_appears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in values_that_are_not_supposed_to_be_zero:\n",
    "    if (i ==0):\n",
    "        print(\"is zero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "they all should match, so now ill do the left merge and bring all the responses, but i still have to extra perfect the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>dataset</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>line_data.csv</td>\n",
       "      <td>What was the price of a barrel of oil in Febru...</td>\n",
       "      <td>$50.24</td>\n",
       "      <td>Retrieve Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>line_data.csv</td>\n",
       "      <td>In which month was the price of a barrel of oi...</td>\n",
       "      <td>December</td>\n",
       "      <td>Find Extremum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>line_data.csv</td>\n",
       "      <td>What was the price range of a barrel of oil in...</td>\n",
       "      <td>$37.04 - $60.95</td>\n",
       "      <td>Determine Range</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>line_data.csv</td>\n",
       "      <td>Over the course of the second half of 2015, th...</td>\n",
       "      <td>falling</td>\n",
       "      <td>Find Correlations/Trends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>line_data.csv</td>\n",
       "      <td>About how much did the price of a barrel of oi...</td>\n",
       "      <td>$15</td>\n",
       "      <td>Make Comparisons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>bar_data.csv</td>\n",
       "      <td>What is the average internet speed in Japan? O...</td>\n",
       "      <td>15 Mbps</td>\n",
       "      <td>Retrieve Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>bar_data.csv</td>\n",
       "      <td>In which country is the average internet speed...</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>Find Extremum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>bar_data.csv</td>\n",
       "      <td>What is the range of the average internet spee...</td>\n",
       "      <td>2 - 20.5 Mbps</td>\n",
       "      <td>Determine Range</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>bar_data.csv</td>\n",
       "      <td>How many countries in Asia is the average inte...</td>\n",
       "      <td>7 countries</td>\n",
       "      <td>Make Comparisons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>stacked_bar_data.csv</td>\n",
       "      <td>What is the cost of peanuts in Las Vegas? Opti...</td>\n",
       "      <td>$12</td>\n",
       "      <td>Retrieve Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>stacked_bar_data.csv</td>\n",
       "      <td>About what is the ratio of the cost of a sandw...</td>\n",
       "      <td>4 to 10</td>\n",
       "      <td>Retrieve Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>stacked_bar_data.csv</td>\n",
       "      <td>In which city is the cost of soda the highest?...</td>\n",
       "      <td>Washington D.C.</td>\n",
       "      <td>Find Extremum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>stacked_bar_data.csv</td>\n",
       "      <td>The ratio of the cost of Soda to the cost of W...</td>\n",
       "      <td>True</td>\n",
       "      <td>Make Comparisons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>100_stacked_bar_data.csv</td>\n",
       "      <td>What is the approval rating of Republicans amo...</td>\n",
       "      <td>38%</td>\n",
       "      <td>Retrieve Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>100_stacked_bar_data.csv</td>\n",
       "      <td>What is the education level of people in which...</td>\n",
       "      <td>College Graduate</td>\n",
       "      <td>Find Extremum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>100_stacked_bar_data.csv</td>\n",
       "      <td>The approval rating of Republicans for the peo...</td>\n",
       "      <td>False</td>\n",
       "      <td>Make Comparisons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>pie_data.csv</td>\n",
       "      <td>About what is the global smartphone market sha...</td>\n",
       "      <td>25%</td>\n",
       "      <td>Retrieve Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>pie_data.csv</td>\n",
       "      <td>In which company is the global smartphone mark...</td>\n",
       "      <td>Lenovo</td>\n",
       "      <td>Find Extremum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>pie_data.csv</td>\n",
       "      <td>The global smartphone market share of Apple is...</td>\n",
       "      <td>True</td>\n",
       "      <td>Make Comparisons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>histogram_data.csv</td>\n",
       "      <td>How many people have rated the taxi between 4....</td>\n",
       "      <td>153</td>\n",
       "      <td>Retrieve Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>histogram_data.csv</td>\n",
       "      <td>What is the rating that the people have rated ...</td>\n",
       "      <td>4.4 - 4.6</td>\n",
       "      <td>Find Extremum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>histogram_data.csv</td>\n",
       "      <td>The distribution of the taxi passenger rating ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Characterize Distribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>histogram_data.csv</td>\n",
       "      <td>More people have rated the taxi between 4.6 an...</td>\n",
       "      <td>True</td>\n",
       "      <td>Make Comparisons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>histogram_data.csv</td>\n",
       "      <td>How many people have rated the taxi 4.9? Optio...</td>\n",
       "      <td>Cannot be inferred</td>\n",
       "      <td>Identify the Characteristic of Bins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>scatterplot_data.csv</td>\n",
       "      <td>What is the weight for the person who is 165.1...</td>\n",
       "      <td>70.5 kg</td>\n",
       "      <td>Retrieve Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>scatterplot_data.csv</td>\n",
       "      <td>What is the height for the tallest person amon...</td>\n",
       "      <td>197.1 cm</td>\n",
       "      <td>Find Extremum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>scatterplot_data.csv</td>\n",
       "      <td>What is the range in weight for the 85 males? ...</td>\n",
       "      <td>53.9 - 123.6 kg</td>\n",
       "      <td>Determine Range</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>scatterplot_data.csv</td>\n",
       "      <td>What is the height for a person who lies outsi...</td>\n",
       "      <td>175.3 cm</td>\n",
       "      <td>Find Anomalies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>scatterplot_data.csv</td>\n",
       "      <td>A group of males are gathered around the heigh...</td>\n",
       "      <td>True</td>\n",
       "      <td>Find Clusters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>scatterplot_data.csv</td>\n",
       "      <td>There is a negative linear relationship betwee...</td>\n",
       "      <td>False</td>\n",
       "      <td>Find Correlations/Trends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>scatterplot_data.csv</td>\n",
       "      <td>The weights for males with the height of 188 c...</td>\n",
       "      <td>False</td>\n",
       "      <td>Make Comparisons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>area_data.csv</td>\n",
       "      <td>What was the average price of a pound of coffe...</td>\n",
       "      <td>$5.1</td>\n",
       "      <td>Retrieve Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>area_data.csv</td>\n",
       "      <td>When was the average price of a pound of coffe...</td>\n",
       "      <td>December 2014</td>\n",
       "      <td>Find Extremum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>area_data.csv</td>\n",
       "      <td>What was the range of the average price of a p...</td>\n",
       "      <td>$4.6 - $6.0</td>\n",
       "      <td>Determine Range</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>area_data.csv</td>\n",
       "      <td>Over the course of 2013, the average price of ...</td>\n",
       "      <td>falling</td>\n",
       "      <td>Find Correlations/Trends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>area_data.csv</td>\n",
       "      <td>For how many months was the average price of a...</td>\n",
       "      <td>3 months</td>\n",
       "      <td>Make Comparisons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>stacked_area_data.csv</td>\n",
       "      <td>What was the number of girls named â€˜Ameliaâ€™ in...</td>\n",
       "      <td>4,200</td>\n",
       "      <td>Retrieve Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>stacked_area_data.csv</td>\n",
       "      <td>About what was the ratio of the number of girl...</td>\n",
       "      <td>1 to 1</td>\n",
       "      <td>Retrieve Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>stacked_area_data.csv</td>\n",
       "      <td>Over the course of years between 2009 and 2014...</td>\n",
       "      <td>2012</td>\n",
       "      <td>Find Extremum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>stacked_area_data.csv</td>\n",
       "      <td>In the UK, the number of girls named â€˜Ameliaâ€™ ...</td>\n",
       "      <td>rising</td>\n",
       "      <td>Make Comparisons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>stacked_area_data.csv</td>\n",
       "      <td>Over the course of years between 2009 and 2014...</td>\n",
       "      <td>False</td>\n",
       "      <td>Make Comparisons</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                   dataset  \\\n",
       "0            0             line_data.csv   \n",
       "1            1             line_data.csv   \n",
       "2            2             line_data.csv   \n",
       "3            3             line_data.csv   \n",
       "4            4             line_data.csv   \n",
       "5            5              bar_data.csv   \n",
       "6            6              bar_data.csv   \n",
       "7            7              bar_data.csv   \n",
       "8            8              bar_data.csv   \n",
       "9            9      stacked_bar_data.csv   \n",
       "10          10      stacked_bar_data.csv   \n",
       "11          11      stacked_bar_data.csv   \n",
       "13          13      stacked_bar_data.csv   \n",
       "14          14  100_stacked_bar_data.csv   \n",
       "15          15  100_stacked_bar_data.csv   \n",
       "16          16  100_stacked_bar_data.csv   \n",
       "17          17              pie_data.csv   \n",
       "18          18              pie_data.csv   \n",
       "19          19              pie_data.csv   \n",
       "20          20        histogram_data.csv   \n",
       "21          21        histogram_data.csv   \n",
       "22          22        histogram_data.csv   \n",
       "23          23        histogram_data.csv   \n",
       "24          24        histogram_data.csv   \n",
       "25          25      scatterplot_data.csv   \n",
       "26          26      scatterplot_data.csv   \n",
       "27          27      scatterplot_data.csv   \n",
       "28          28      scatterplot_data.csv   \n",
       "29          29      scatterplot_data.csv   \n",
       "30          30      scatterplot_data.csv   \n",
       "31          31      scatterplot_data.csv   \n",
       "32          32             area_data.csv   \n",
       "33          33             area_data.csv   \n",
       "34          34             area_data.csv   \n",
       "35          35             area_data.csv   \n",
       "36          36             area_data.csv   \n",
       "37          37     stacked_area_data.csv   \n",
       "38          38     stacked_area_data.csv   \n",
       "39          39     stacked_area_data.csv   \n",
       "41          41     stacked_area_data.csv   \n",
       "42          42     stacked_area_data.csv   \n",
       "\n",
       "                                             question                answer  \\\n",
       "0   What was the price of a barrel of oil in Febru...                $50.24   \n",
       "1   In which month was the price of a barrel of oi...              December   \n",
       "2   What was the price range of a barrel of oil in...      $37.04 - $60.95    \n",
       "3   Over the course of the second half of 2015, th...               falling   \n",
       "4   About how much did the price of a barrel of oi...                   $15   \n",
       "5   What is the average internet speed in Japan? O...               15 Mbps   \n",
       "6   In which country is the average internet speed...           South Korea   \n",
       "7   What is the range of the average internet spee...        2 - 20.5 Mbps    \n",
       "8   How many countries in Asia is the average inte...           7 countries   \n",
       "9   What is the cost of peanuts in Las Vegas? Opti...                   $12   \n",
       "10  About what is the ratio of the cost of a sandw...              4 to 10    \n",
       "11  In which city is the cost of soda the highest?...       Washington D.C.   \n",
       "13  The ratio of the cost of Soda to the cost of W...                  True   \n",
       "14  What is the approval rating of Republicans amo...                   38%   \n",
       "15  What is the education level of people in which...      College Graduate   \n",
       "16  The approval rating of Republicans for the peo...                False    \n",
       "17  About what is the global smartphone market sha...                   25%   \n",
       "18  In which company is the global smartphone mark...               Lenovo    \n",
       "19  The global smartphone market share of Apple is...                  True   \n",
       "20  How many people have rated the taxi between 4....                   153   \n",
       "21  What is the rating that the people have rated ...            4.4 - 4.6    \n",
       "22  The distribution of the taxi passenger rating ...                 True    \n",
       "23  More people have rated the taxi between 4.6 an...                 True    \n",
       "24  How many people have rated the taxi 4.9? Optio...   Cannot be inferred    \n",
       "25  What is the weight for the person who is 165.1...               70.5 kg   \n",
       "26  What is the height for the tallest person amon...              197.1 cm   \n",
       "27  What is the range in weight for the 85 males? ...       53.9 - 123.6 kg   \n",
       "28  What is the height for a person who lies outsi...             175.3 cm    \n",
       "29  A group of males are gathered around the heigh...                 True    \n",
       "30  There is a negative linear relationship betwee...                 False   \n",
       "31  The weights for males with the height of 188 c...                 False   \n",
       "32  What was the average price of a pound of coffe...                  $5.1   \n",
       "33  When was the average price of a pound of coffe...         December 2014   \n",
       "34  What was the range of the average price of a p...           $4.6 - $6.0   \n",
       "35  Over the course of 2013, the average price of ...               falling   \n",
       "36  For how many months was the average price of a...             3 months    \n",
       "37  What was the number of girls named â€˜Ameliaâ€™ in...                 4,200   \n",
       "38  About what was the ratio of the number of girl...                1 to 1   \n",
       "39  Over the course of years between 2009 and 2014...                  2012   \n",
       "41  In the UK, the number of girls named â€˜Ameliaâ€™ ...                rising   \n",
       "42  Over the course of years between 2009 and 2014...                 False   \n",
       "\n",
       "                          question_type  \n",
       "0                        Retrieve Value  \n",
       "1                         Find Extremum  \n",
       "2                       Determine Range  \n",
       "3              Find Correlations/Trends  \n",
       "4                      Make Comparisons  \n",
       "5                        Retrieve Value  \n",
       "6                         Find Extremum  \n",
       "7                       Determine Range  \n",
       "8                      Make Comparisons  \n",
       "9                        Retrieve Value  \n",
       "10                       Retrieve Value  \n",
       "11                        Find Extremum  \n",
       "13                     Make Comparisons  \n",
       "14                       Retrieve Value  \n",
       "15                        Find Extremum  \n",
       "16                     Make Comparisons  \n",
       "17                       Retrieve Value  \n",
       "18                        Find Extremum  \n",
       "19                     Make Comparisons  \n",
       "20                       Retrieve Value  \n",
       "21                        Find Extremum  \n",
       "22            Characterize Distribution  \n",
       "23                     Make Comparisons  \n",
       "24  Identify the Characteristic of Bins  \n",
       "25                       Retrieve Value  \n",
       "26                        Find Extremum  \n",
       "27                      Determine Range  \n",
       "28                       Find Anomalies  \n",
       "29                        Find Clusters  \n",
       "30             Find Correlations/Trends  \n",
       "31                     Make Comparisons  \n",
       "32                       Retrieve Value  \n",
       "33                        Find Extremum  \n",
       "34                      Determine Range  \n",
       "35             Find Correlations/Trends  \n",
       "36                     Make Comparisons  \n",
       "37                       Retrieve Value  \n",
       "38                       Retrieve Value  \n",
       "39                        Find Extremum  \n",
       "41                     Make Comparisons  \n",
       "42                     Make Comparisons  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "heres the generation moment for this df, so i can keep generating it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "answered_questions_and_correct_values = pd.merge(ready_filled_merged_df, new_ans, on='question', how='left')\n",
    "answered_questions_and_correct_values=answered_questions_and_correct_values.drop([\"dataset_x\", \"dataset_y\", 'Unnamed: 0_y', 'Unnamed: 0_x'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now ill just use the following function to get how many were right, to generate the pass@k metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image', 'question', 'gpt_answer_1', 'gpt_answer_2', 'gpt_answer_3',\n",
       "       'gpt_answer_4', 'gpt_answer_5', 'gpt_answer_6', 'gpt_answer_7',\n",
       "       'gpt_answer_8', 'gpt_answer_9', 'gpt_answer_10', 'gpt_answer_11',\n",
       "       'gpt_answer_12', 'gpt_answer_13', 'gpt_answer_14', 'gpt_answer_15',\n",
       "       'gpt_answer_16', 'gpt_answer_17', 'gpt_answer_18', 'gpt_answer_19',\n",
       "       'gpt_answer_20', 'worsened_axes', 'google_model', 'openai_model',\n",
       "       'title', 'referential_title', 'normal_title',\n",
       "       'dataset_100_stacked_bar_data.csv', 'dataset_area_data.csv',\n",
       "       'dataset_bar_data.csv', 'dataset_histogram_data.csv',\n",
       "       'dataset_line_data.csv', 'dataset_pie_data.csv',\n",
       "       'dataset_scatterplot_data.csv', 'dataset_stacked_area_data.csv',\n",
       "       'dataset_stacked_bar_data.csv', 'plot_type_bar', 'plot_type_hist',\n",
       "       'plot_type_line', 'plot_type_pie', 'plot_type_scatter',\n",
       "       'plot_type_stacked_bar', 'plot_type_stacked_bar_100', 'color_black',\n",
       "       'color_burlywood', 'color_chartreuse', 'color_lightgray',\n",
       "       'color_lightpink', 'color_mediumvioletred', 'color_navy', 'color_red',\n",
       "       'color_saddlebrown', 'color_tomato', 'answer', 'question_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answered_questions_and_correct_values.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before counting how many they get right outta 20, lets make it more fair, and take out spaces, and lower every letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['gpt_answer_1', 'gpt_answer_2', 'gpt_answer_3',\n",
    "       'gpt_answer_4', 'gpt_answer_5', 'gpt_answer_6', 'gpt_answer_7',\n",
    "       'gpt_answer_8', 'gpt_answer_9', 'gpt_answer_10', 'gpt_answer_11',\n",
    "       'gpt_answer_12', 'gpt_answer_13', 'gpt_answer_14', 'gpt_answer_15',\n",
    "       'gpt_answer_16', 'gpt_answer_17', 'gpt_answer_18', 'gpt_answer_19',\n",
    "       'gpt_answer_20', 'answer']:\n",
    "      if col in answered_questions_and_correct_values.columns: #Check that the column exists.\n",
    "        answered_questions_and_correct_values[col] = answered_questions_and_correct_values[col].astype(str).str.lower().str.replace(r'\\s+', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now a function to count the times it got it right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_correct_count_column(df, answer_columns, correct_answer_column, new_column_name='correct_count'):\n",
    "    \"\"\"\n",
    "    Adds a new column to the DataFrame containing the number of answer columns that match the correct answer.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to modify.\n",
    "        answer_columns (list): A list of column names representing answer choices.\n",
    "        correct_answer_column (str): The name of the column containing the correct answer.\n",
    "        new_column_name (str): The name of the new column to create. Defaults to 'correct_count'.\n",
    "    \"\"\"\n",
    "\n",
    "    def count_correct(row):\n",
    "        correct_answer = row[correct_answer_column]\n",
    "        correct_count = 0\n",
    "        for col in answer_columns:\n",
    "            if row[col] == correct_answer:\n",
    "                correct_count += 1\n",
    "        return correct_count\n",
    "\n",
    "    df[new_column_name] = df.apply(count_correct, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so this one below tracks how often they get it right out of 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_correct_count_column(answered_questions_and_correct_values, ['gpt_answer_1', 'gpt_answer_2', 'gpt_answer_3',\n",
    "       'gpt_answer_4', 'gpt_answer_5', 'gpt_answer_6', 'gpt_answer_7',\n",
    "       'gpt_answer_8', 'gpt_answer_9', 'gpt_answer_10', 'gpt_answer_11',\n",
    "       'gpt_answer_12', 'gpt_answer_13', 'gpt_answer_14', 'gpt_answer_15',\n",
    "       'gpt_answer_16', 'gpt_answer_17', 'gpt_answer_18', 'gpt_answer_19',\n",
    "       'gpt_answer_20'], 'answer','passATk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "answered_questions_and_correct_values.to_csv(\"goodResults.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
